# backend/main.py
from fastapi import FastAPI, Depends, HTTPException, Request, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sqlalchemy.orm import Session
from datetime import datetime, timedelta
from jose import jwt, JWTError
from passlib.context import CryptContext
from zhipuai import ZhipuAI
import simplejson as json
import random
import jieba
import jieba.analyse
import jieba.posseg as pseg
from collections import Counter
import re
import hashlib
import requests
import time
import traceback
import calendar
from datetime import datetime
import config
from models import Base, SessionLocal, User, ReasoningRecord, LoginLog
# ---------------------- äº‹å®æŸ¥è¯¢æ¨¡å— ----------------------
class FactChecker:
    """äº‹å®æ£€æŸ¥å™¨ï¼Œå¤„ç†ç®€å•äº‹å®æŸ¥è¯¢"""
    
    @staticmethod
    def check_simple_facts(content: str) -> dict:
        """
        æ£€æŸ¥ç®€å•çš„äº‹å®æ€§é™ˆè¿°
        è¿”å›ï¼š{"is_factual": bool, "correction": str, "certainty": float}
        """
        content_lower = content.lower()
        
        # æ£€æŸ¥æ—¥æœŸç›¸å…³
        current_time = datetime.now()
        current_year = current_time.year
        current_month = current_time.month
        current_day = current_time.day
        current_weekday = current_time.weekday()  # 0=Monday, 6=Sunday
        
        weekdays_chinese = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"]
        current_weekday_chinese = weekdays_chinese[current_weekday]
        
        # å¸¸è§æ—¥æœŸæ¨¡å¼åŒ¹é…
        date_patterns = [
            (r'ä»Šå¤©.*æ˜ŸæœŸ[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]', f"ä»Šå¤©æ˜¯{current_weekday_chinese}"),
            (r'ä»Šå¤©.*å‘¨[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]', f"ä»Šå¤©æ˜¯{current_weekday_chinese}"),
            (r'ä»Šå¤©æ˜¯.*æ˜ŸæœŸå‡ ', f"ä»Šå¤©æ˜¯{current_weekday_chinese}"),
            (r'ç°åœ¨.*æ˜ŸæœŸ[ä¸€äºŒä¸‰å››äº”å…­æ—¥å¤©]', f"ç°åœ¨æ˜¯{current_weekday_chinese}"),
            (r'ä»Šå¤©æ˜¯\d+å¹´\d+æœˆ\d+æ—¥', f"ä»Šå¤©æ˜¯{current_year}å¹´{current_month}æœˆ{current_day}æ—¥"),
            (r'ç°åœ¨æ˜¯\d+å¹´', f"ç°åœ¨æ˜¯{current_year}å¹´"),
            (r'ä»Šå¹´.*\d+å²', None),  # å¹´é¾„ç›¸å…³ï¼Œéœ€è¦æ›´å¤æ‚å¤„ç†
        ]
        
        for pattern, correction in date_patterns:
            if re.search(pattern, content_lower):
                if correction:
                    return {
                        "is_factual": content_lower in correction.lower(),
                        "correction": correction,
                        "certainty": 1.0,
                        "fact_type": "æ—¥æœŸæ—¶é—´"
                    }
        
        # æ£€æŸ¥å¸¸è¯†æ€§äº‹å®
        common_facts = {
            "å¤ªé˜³ä»ä¸œè¾¹å‡èµ·": True,
            "åœ°çƒæ˜¯åœ†çš„": True,
            "æ°´åœ¨0æ‘„æ°åº¦ç»“å†°": True,
            "1+1ç­‰äº2": True,
            "äººéœ€è¦å‘¼å¸æ°§æ°”": True,
            "é±¼ç”Ÿæ´»åœ¨æ°´é‡Œ": True,
            "é¸Ÿä¼šé£": True,
        }
        
        for fact, is_true in common_facts.items():
            if fact in content:
                return {
                    "is_factual": is_true,
                    "correction": f"{fact}æ˜¯{'' if is_true else 'ä¸'}æ­£ç¡®çš„",
                    "certainty": 0.95,
                    "fact_type": "å¸¸è¯†"
                }
        
        # æ£€æŸ¥æ˜æ˜¾é”™è¯¯çš„å¸¸è¯†
        false_facts = {
            "å¤ªé˜³ä»è¥¿è¾¹å‡èµ·": "å¤ªé˜³ä»ä¸œè¾¹å‡èµ·",
            "åœ°çƒæ˜¯å¹³çš„": "åœ°çƒæ˜¯è¿‘ä¼¼çƒä½“",
            "æ°´åœ¨100æ‘„æ°åº¦ç»“å†°": "æ°´åœ¨0æ‘„æ°åº¦ç»“å†°ï¼Œ100æ‘„æ°åº¦æ²¸è…¾",
            "1+1ç­‰äº3": "1+1ç­‰äº2",
            "äººä¸éœ€è¦æ°§æ°”": "äººç±»éœ€è¦æ°§æ°”è¿›è¡Œå‘¼å¸",
        }
        
        for false_fact, correction in false_facts.items():
            if false_fact in content:
                return {
                    "is_factual": False,
                    "correction": f"æ­£ç¡®è¯´æ³•æ˜¯ï¼š{correction}",
                    "certainty": 0.99,
                    "fact_type": "å¸¸è¯†çº é”™"
                }
        
        return {
            "is_factual": None,
            "correction": "",
            "certainty": 0.0,
            "fact_type": "æ— æ³•åˆ¤æ–­"
        }
# ---------------------- DuckDuckGo æœç´¢å®¢æˆ·ç«¯ ----------------------
try:
    from duckduckgo_search import DDGS
    DUCKDUCKGO_AVAILABLE = True
except ImportError:
    DUCKDUCKGO_AVAILABLE = False
    print("âš  duckduckgo-search æœªå®‰è£…ï¼Œè”ç½‘æœç´¢åŠŸèƒ½å°†ä¸å¯ç”¨")

class DuckDuckGoSearchClient:
    """DuckDuckGo æœç´¢å®¢æˆ·ç«¯"""
    
    def __init__(self, timeout: int = 15):
        self.timeout = timeout
        self.max_results = config.SEARCH_CONFIG.get("max_results", 3)
        self.cooldown = config.SEARCH_CONFIG.get("cooldown", 0.5)
    
    def search(self, query: str, max_results: int = None) -> dict:
        """æ‰§è¡ŒDuckDuckGoæœç´¢"""
        if not DUCKDUCKGO_AVAILABLE:
            return {"results": [], "query": query, "success": False, "error": "DuckDuckGoä¸å¯ç”¨"}
        
        try:
            max_results = max_results or self.max_results
            print(f"ğŸ” DuckDuckGoæœç´¢: {query}")
            
            results = []
            with DDGS() as ddgs:
                for r in ddgs.text(query, max_results=max_results):
                    results.append({
                        "title": r.get("title", ""),
                        "content": r.get("body", ""),
                        "url": r.get("href", ""),
                        "source": "DuckDuckGo"
                    })
            
            return {
                "results": results,
                "query": query,
                "success": len(results) > 0,
                "count": len(results)
            }
            
        except Exception as e:
            print(f"âŒ DuckDuckGoæœç´¢å¤±è´¥: {str(e)}")
            return {"results": [], "query": query, "success": False, "error": str(e)}
    
    def search_for_rumor_verification(self, content: str, keywords: list) -> dict:
        """ä¸ºè°£è¨€éªŒè¯è®¾è®¡çš„æœç´¢"""
        search_queries = self._generate_rumor_queries(content, keywords)
        all_results = []
        
        for query in search_queries[:config.SEARCH_CONFIG.get("max_queries", 2)]:
            search_result = self.search(query, max_results=2)
            
            if search_result.get("success") and search_result["results"]:
                for result in search_result["results"]:
                    formatted_result = self._format_search_result(result)
                    if formatted_result:
                        all_results.append(formatted_result)
            
            time.sleep(self.cooldown)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        return {
            "query_count": len(search_queries),
            "total_results": len(all_results),
            "results": all_results[:6],  # æœ€å¤šè¿”å›6ä¸ªç»“æœ
            "success": len(all_results) > 0
        }
    
    def _generate_rumor_queries(self, content: str, keywords: list) -> list:
        """ç”Ÿæˆè°£è¨€éªŒè¯æŸ¥è¯¢"""
        queries = []
        
        # åŸºäºå…³é”®å­—çš„æŸ¥è¯¢
        if keywords:
            main_keywords = " ".join(keywords[:2])
            queries.extend([
                f"{main_keywords} è°£è¨€ è¾Ÿè°£",
                f"{main_keywords} äº‹å®æ ¸æŸ¥",
                f"{main_keywords} æ˜¯çœŸçš„å—",
                f"{main_keywords} çœŸç›¸"
            ])
        
        # åŸºäºå†…å®¹çš„æŸ¥è¯¢
        content_lower = content.lower()
        
        # æå–çŸ­å¥ä½œä¸ºæŸ¥è¯¢
        if len(content) < 100:
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
            for sentence in sentences:
                sentence = sentence.strip()
                if 10 < len(sentence) < 50:
                    queries.append(f"{sentence} æ˜¯çœŸçš„å—")
        
        # ç‰¹å®šä¸»é¢˜çš„æŸ¥è¯¢
        if any(word in content_lower for word in ["ç–«æƒ…", "ç–«è‹—", "æ–°å† ", "ç—…æ¯’"]):
            queries.extend([
                "ç–«æƒ…è°£è¨€ å®˜æ–¹è¾Ÿè°£",
                "æ–°å† ç–«è‹— çœŸç›¸"
            ])
        if any(word in content_lower for word in ["é£Ÿå“", "åƒ", "å–", "ä¸­æ¯’", "è‡´ç™Œ"]):
            queries.append("é£Ÿå“å®‰å…¨è°£è¨€ è¾Ÿè°£")
        if any(word in content_lower for word in ["å¥åº·", "å…»ç”Ÿ", "æ²»ç—…", "åæ–¹"]):
            queries.append("å¥åº·è°£è¨€ çœŸç›¸")
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        return list(dict.fromkeys(queries))[:4]
    
    def _format_search_result(self, result: dict) -> dict:
        """æ ¼å¼åŒ–æœç´¢ç»“æœ"""
        try:
            title = result.get("title", "").strip()
            content = result.get("content", "").strip()
            url = result.get("url", "")
            source = result.get("source", "DuckDuckGo")
            
            if not content:
                return None
            
            # æå–å…³é”®ä¿¡æ¯
            summary = content[:100] + "..." if len(content) > 100 else content
            
            # æ£€æµ‹ç»“æœç±»å‹
            result_type = "æ™®é€šä¿¡æ¯"
            if any(word in content for word in ["è¾Ÿè°£", "è°£è¨€", "ä¸å®", "è™šå‡"]):
                result_type = "è¾Ÿè°£ä¿¡æ¯"
            elif any(word in content for word in ["è¯å®", "çœŸç›¸", "äº‹å®", "æ­£ç¡®"]):
                result_type = "è¯å®ä¿¡æ¯"
            elif any(word in content for word in ["å¯èƒ½", "æˆ–è®¸", "ä¸ç¡®å®š", "ç–‘ä¼¼"]):
                result_type = "ä¸ç¡®å®šä¿¡æ¯"
            
            return {
                "title": title,
                "summary": summary,
                "full_content": content,
                "url": url,
                "source": source,
                "type": result_type,
                "relevance_score": self._calculate_relevance(content)
            }
            
        except Exception as e:
            print(f"æ ¼å¼åŒ–æœç´¢ç»“æœå¤±è´¥: {str(e)}")
            return None
    
    def _calculate_relevance(self, content: str) -> float:
        """è®¡ç®—æœç´¢ç»“æœç›¸å…³æ€§åˆ†æ•°"""
        relevance_keywords = [
            "è¾Ÿè°£", "è°£è¨€", "è¯å®", "çœŸç›¸", "äº‹å®", "æ ¸æŸ¥",
            "ä¸“å®¶", "ç ”ç©¶", "å®éªŒ", "æ•°æ®", "ç§‘å­¦", "å®˜æ–¹"
        ]
        
        score = 0.5  # åŸºç¡€åˆ†
        
        content_lower = content.lower()
        for keyword in relevance_keywords:
            if keyword in content_lower:
                score += 0.1
        
        # é™åˆ¶åœ¨0-1ä¹‹é—´
        return min(max(score, 0), 1)

# ---------------------- åŸºç¡€é…ç½® ----------------------
app = FastAPI(title="è°£è¨€ç”„åˆ«ç³»ç»ŸAPI")

# è·¨åŸŸé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–æ•°æ®åº“
from models import SessionLocal

# å¯†ç åŠ å¯†å·¥å…·
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

# ---------------------- åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯ ----------------------
from zhipuai import ZhipuAI
import config
llm_client = ZhipuAI(
    api_key=config.LLM_CONFIG["api_key"],
    base_url=config.LLM_CONFIG.get("base_url", "https://open.bigmodel.cn/api/coding/paas/v4")
)

# ---------------------- åˆå§‹åŒ–æœç´¢å®¢æˆ·ç«¯ ----------------------
search_client = DuckDuckGoSearchClient()

# ---------------------- ä½¿ç”¨jiebaæå–å…³é”®å­—çš„å‡½æ•° ----------------------
def extract_keywords_with_jieba(content: str, top_k: int = 8) -> list:
    """
    ä½¿ç”¨jiebaç²¾ç¡®æ¨¡å¼æå–æ–‡æœ¬çš„å…³é”®å­—
    å‚æ•°:
        content: è¾“å…¥æ–‡æœ¬
        top_k: è¿”å›å…³é”®è¯æ•°é‡
    è¿”å›:
        å…³é”®å­—åˆ—è¡¨
    """
    if not content or len(content.strip()) == 0:
        return ["æœªçŸ¥"]
    
    original_content = content
    
    # æ¸…æ´—æ–‡æœ¬
    important_words = {"ä¸", "æ²¡", "æ— ", "å¦", "é", "æœª", "å‹¿", "è«", "ä¼‘", "å¿Œ", "ç¦", "æˆ’", "å°±", "æ‰€ä»¥", "å› æ­¤", "å› è€Œ", "ä»è€Œ"}
    
    placeholder_map = {}
    for i, word in enumerate(important_words):
        placeholder = f"__PLACEHOLDER_{i}__"
        placeholder_map[placeholder] = word
        content = content.replace(word, placeholder)
    
    cleaned_content = re.sub(r'[^\w\s]', '', content)
    
    for placeholder, word in placeholder_map.items():
        cleaned_content = cleaned_content.replace(placeholder, word)
    
    # ä½¿ç”¨ç²¾ç¡®æ¨¡å¼åˆ†è¯
    words = jieba.lcut(cleaned_content, cut_all=False)
    
    # è¿‡æ»¤é€»è¾‘
    base_stop_words = {"çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "éƒ½", "ä¸€", "ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™"}
    
    important_negations = {"ä¸", "æ²¡", "æ— ", "å¦", "é", "æœª", "å‹¿", "è«", "ä¼‘", "å¿Œ", "ç¦", "æˆ’", "ä¸æ˜¯", "ä¸ä¼š", "ä¸èƒ½", "ä¸å¯", "æ²¡æœ‰", "æ— æ³•"}
    important_logicals = {"æ‰€ä»¥", "å› æ­¤", "å› è€Œ", "ä»è€Œ", "å› ä¸º", "ç”±äº", "æ—¢ç„¶", "é‚£ä¹ˆ", "äºæ˜¯", "ç„¶å"}
    
    must_keep_words = important_negations.union(important_logicals)
    
    filtered_words = []
    for word in words:
        if word in must_keep_words:
            filtered_words.append(word)
        elif word in base_stop_words:
            continue
        elif len(word) == 1 and word not in important_negations:
            continue
        else:
            filtered_words.append(word)
    
    # ç»Ÿè®¡è¯é¢‘
    word_freq = Counter(filtered_words)
    
    # è·å–å‰top_kä¸ªé«˜é¢‘è¯
    keywords = [word for word, _ in word_freq.most_common(top_k)]
    
    # å¦‚æœæå–çš„å…³é”®è¯ä¸è¶³ï¼Œä½¿ç”¨å…³é”®çŸ­è¯­æå–
    if len(keywords) < min(5, top_k):
        try:
            tfidf_keywords = jieba.analyse.extract_tags(
                original_content, 
                topK=top_k*2, 
                withWeight=False,
                allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'd')
            )
            
            for keyword in tfidf_keywords:
                if keyword in must_keep_words and keyword not in keywords:
                    keywords.append(keyword)
        except:
            pass
    
    # ç‰¹åˆ«å¤„ç†å¦å®š+å…³é”®è¯çš„ç»„åˆ
    negation_patterns = [
        r'ä¸\s*([^\s]+)',
        r'æ²¡\s*([^\s]+)',
        r'æ— \s*([^\s]+)',
        r'å¦\s*([^\s]+)',
        r'é\s*([^\s]+)',
        r'ä¸æ˜¯\s*([^\s]+)',
        r'æ²¡æœ‰\s*([^\s]+)',
    ]
    
    for pattern in negation_patterns:
        matches = re.findall(pattern, original_content)
        for match in matches:
            if len(match) > 1:
                negation_word = pattern.split(r'\s*')[0].replace('r', '').replace("'", "")
                combined = negation_word + match
                if combined not in keywords:
                    keywords.append(combined)
    
    # ä½¿ç”¨è¯æ€§æ ‡æ³¨æå–æ›´å¤šä¿¡æ¯
    try:
        word_flags = pseg.lcut(original_content)
        meaningful_words = []
        for word, flag in word_flags:
            if flag.startswith(('n', 'v', 'a')) and len(word) > 1:
                meaningful_words.append(word)
        
        for word in meaningful_words:
            if word not in keywords:
                keywords.append(word)
    except:
        pass
    
    # å»é‡
    unique_keywords = []
    seen = set()
    for word in keywords:
        if word and word not in seen:
            seen.add(word)
            unique_keywords.append(word)
    
    if not unique_keywords:
        unique_keywords = ["ä¿¡æ¯ä¸è¶³"]
    
    return unique_keywords[:top_k]

# ---------------------- å¤§è¯­è¨€æ¨¡å‹æç¤ºè¯æ¨¡æ¿ ----------------------
# å¢å¼ºçš„æç¤ºè¯æ¨¡æ¿
ENHANCED_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è°£è¨€ç”„åˆ«ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œåˆ†æï¼š

=== å¾…æ£€æµ‹ä¿¡æ¯ ===
æ–‡æœ¬å†…å®¹ï¼š{content}
æ–‡æœ¬ç±»å‹ï¼š{type}
å…³é”®è¯ï¼š{keywords}

=== ç½‘ç»œæœç´¢ç»“æœ ===
{search_summary}

=== åˆ†æè¦æ±‚ ===
1. é¦–å…ˆåˆ†ææ–‡æœ¬ä¸­çš„æ ¸å¿ƒå£°æ˜ï¼ŒåŒºåˆ†ä¿¡æ¯æ€§è´¨ï¼šåˆ¤æ–­å®ƒæ˜¯"ä¸€ä¸ªéœ€è¦æ ¸å®çš„æ–°ä¼ è¨€"ï¼Œè¿˜æ˜¯ä¸€ä¸ª"å¯¹æ—¢æœ‰äº‹å®çš„é™ˆè¿°"ã€‚
2. å‚è€ƒæœç´¢ç»“æœä¸­çš„ä¿¡æ¯è¿›è¡Œäº‹å®æ ¸æŸ¥
3. è¯„ä¼°å£°æ˜çš„é€»è¾‘ä¸€è‡´æ€§å’Œåˆç†æ€§
4. ç»¼åˆæœç´¢ç»“æœå’Œé€»è¾‘åˆ†æç»™å‡ºåˆ¤æ–­
å¯¹äº"æ—¢æœ‰äº‹å®é™ˆè¿°"ï¼Œç‰¹åˆ«æ˜¯åŒ…å«ä»¥ä¸‹ç‰¹å¾çš„ä¿¡æ¯ï¼Œåº”å€¾å‘äºè®¤ä¸ºå…¶å¯ä¿¡ï¼š
   - åŒ…å«æ˜ç¡®çš„æ—¶é—´ï¼ˆå¦‚"2025å¹´8æœˆ"ï¼‰ã€åœ°ç‚¹ï¼ˆå¦‚"æˆéƒ½"ï¼‰ã€æœºæ„åç§°ï¼ˆå¦‚"å›½å®¶èˆªå¤©å±€"ï¼‰ã€‚
   - æè¿°çš„æ˜¯å·²å®Œæˆçš„ã€æœ‰å®˜æ–¹è®°å½•çš„å…¬å…±äº‹ä»¶ï¼ˆå¦‚å·²ä¸¾åŠçš„èµ›äº‹ã€å·²å‘å¸ƒçš„å›½å®¶æ”¿ç­–ã€å·²å®Œæˆçš„ç§‘å­¦ä»»åŠ¡ï¼‰ã€‚
   - è¯­è¨€é£æ ¼å®¢è§‚ã€å¹³å®ï¼Œç¬¦åˆæ–°é—»æŠ¥é“ç‰¹å¾ã€‚
å¯¹äºç¬¦åˆä¸Šè¿°ç‰¹å¾çš„"æ—¢æœ‰äº‹å®"ï¼Œåº”ä¼˜å…ˆé€šè¿‡ç½‘ç»œæœç´¢æˆ–å¸¸è¯†è¿›è¡ŒéªŒè¯ï¼Œè€Œéç›´æ¥è´¨ç–‘å…¶çœŸå®æ€§ã€‚
è¯„ä¼°é€»è¾‘æ—¶ï¼Œéœ€è€ƒè™‘è¯¥äº‹ä»¶å‘ç”Ÿçš„åˆç†æ€§ä¸æ˜¯å¦ç¬¦åˆå…¬å¼€æ—¥ç¨‹ã€‚
=== æ¦‚ç‡è®¡ç®—æ ‡å‡† ===
è¯·æ ¹æ®è¯æ®å¼ºåº¦ç»™å‡ºç²¾ç¡®çš„æ¦‚ç‡å€¼ï¼ˆ0.0000-1.0000ï¼‰ï¼š
- 0.9000-1.0000: æœ‰ç¡®å‡¿è¯æ®è¯æ˜æ˜¯è°£è¨€
- 0.7000-0.9000: æœ‰è¾ƒå¼ºè¯æ®è¡¨æ˜æ˜¯è°£è¨€
- 0.5000-0.7000: å¯èƒ½æ˜¯è°£è¨€ï¼Œè¯æ®ä¸è¶³
- 0.3000-0.5000: å¯èƒ½ä¸æ˜¯è°£è¨€
- 0.1000-0.3000: å¾ˆå¯èƒ½ä¸æ˜¯è°£è¨€
- 0.0000-0.1000: æœ‰ç¡®å‡¿è¯æ®è¯æ˜ä¸æ˜¯è°£è¨€

**é‡è¦ï¼šä¸è¦å›ºå®šä½¿ç”¨0.8500æˆ–0.1500ï¼Œæ ¹æ®è¯æ®å¼ºåº¦åŠ¨æ€è°ƒæ•´**

=== è¾“å‡ºæ ¼å¼ ===
ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "reasoning_steps": [
    "ç¬¬ä¸€æ­¥ï¼šåˆ†ææ–‡æœ¬æ ¸å¿ƒå£°æ˜",
    "ç¬¬äºŒæ­¥ï¼šæ ¸æŸ¥æœç´¢ç»“æœä¸­çš„äº‹å®ä¾æ®", 
    "ç¬¬ä¸‰æ­¥ï¼šè¯„ä¼°é€»è¾‘å’Œåˆç†æ€§",
    "ç¬¬å››æ­¥ï¼šç»¼åˆç»™å‡ºåˆ¤æ–­ç»“è®º"
  ],
  "is_ai_generated": false,
  "rumor_prob": 0.7236,  #  æ³¨æ„ï¼šè¿™æ˜¯ç¤ºä¾‹ï¼Œè¯·æ ¹æ®å®é™…åˆ†æè®¡ç®—
  "is_rumor": true,
  "conclusion": "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€æ˜¯è°£è¨€ã€‘ã€‚",
  "confidence": "é«˜/ä¸­/ä½",
  "verification_based_on_search": true/false,
  "search_result_summary": "å¯¹æœç´¢ç»“æœçš„ç®€è¦æ€»ç»“",
  "key_findings_from_search": ["å‘ç°1", "å‘ç°2"]
}}
"""

# åŸå§‹æç¤ºè¯æ¨¡æ¿ï¼ˆæ— æœç´¢ç»“æœï¼‰
PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è°£è¨€ç”„åˆ«ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–è¯´æ˜ï¼š

{{
  "reasoning_steps": [
    "ç¬¬ä¸€æ­¥ï¼šè¯†åˆ«å’Œåˆ†ææ–‡æœ¬å†…å®¹",
    "ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥äº‹å®å’Œé€»è¾‘ä¸€è‡´æ€§",
    "ç¬¬ä¸‰æ­¥ï¼šè¯„ä¼°å¯ä¿¡åº¦å’Œåˆç†æ€§",
    "ç¬¬å››æ­¥ï¼šç»™å‡ºæœ€ç»ˆåˆ¤æ–­ç»“è®º"
  ],
  "is_ai_generated": false,
  "rumor_prob": 0.8500,
  "is_rumor": true,
  "conclusion": "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€æ˜¯è°£è¨€ã€‘ã€‚"
}}

=== è¾“å…¥ä¿¡æ¯ ===
æ–‡æœ¬å†…å®¹ï¼š{content}
æ–‡æœ¬ç±»å‹ï¼š{type}
å…³é”®è¯ï¼š{keywords}

=== åˆ†æè¦æ±‚ ===
1. è¯·åŸºäºæä¾›çš„æ–‡æœ¬å†…å®¹ï¼ŒæŒ‰ç…§4æ­¥æ¨ç†æµç¨‹è¿›è¡Œåˆ†æ
2. reasoning_stepså¿…é¡»åŒ…å«4ä¸ªæ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤ç”¨ä¸€å¥ç®€æ´æ˜äº†çš„è¯æè¿°
3. is_ai_generatedåˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºAIç”Ÿæˆï¼štrueï¼ˆæ˜¯ï¼‰æˆ–falseï¼ˆå¦ï¼‰
4. rumor_probç»™å‡ºè°£è¨€æ¦‚ç‡ï¼š0-1ä¹‹é—´çš„4ä½å°æ•°ï¼Œ0è¡¨ç¤ºè‚¯å®šæ˜¯è°£è¨€ï¼Œ1è¡¨ç¤ºè‚¯å®šä¸æ˜¯è°£è¨€
5. is_rumoråˆ¤æ–­æ˜¯å¦ä¸ºè°£è¨€ï¼štrueè¡¨ç¤ºæ˜¯è°£è¨€ï¼Œfalseè¡¨ç¤ºä¸æ˜¯è°£è¨€
6. conclusionç»™å‡ºæœ€ç»ˆç»“è®ºï¼šå¿…é¡»æ˜ç¡®åŒ…å«ã€æ˜¯è°£è¨€ã€‘æˆ–ã€ä¸æ˜¯è°£è¨€ã€‘
7. è¯·ç¡®ä¿åˆ†æå®¢è§‚ã€å‡†ç¡®ï¼ŒåŸºäºäº‹å®å’Œé€»è¾‘

=== è¾“å‡ºæ ¼å¼è¦æ±‚ ===
åªè¿”å›JSONæ ¼å¼çš„è¾“å‡ºï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—è¯´æ˜ã€æ³¨é‡Šæˆ–æ ¼å¼æ ‡è®°ã€‚
JSONå¿…é¡»åŒ…å«ä¸”ä»…åŒ…å«ä»¥ä¸‹å­—æ®µï¼šreasoning_steps, is_ai_generated, rumor_prob, is_rumor, conclusion
=== æ¦‚ç‡è®¡ç®—æŒ‡å— ===
è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†ç»™å‡ºæ¦‚ç‡å€¼ï¼š
- 0.9-1.0: å‡ ä¹è‚¯å®šæ˜¯è°£è¨€ï¼Œæœ‰æ˜æ˜¾è¯æ®
- 0.7-0.9: å¾ˆå¯èƒ½æ˜¯è°£è¨€ï¼Œæœ‰è¾ƒå¼ºè¯æ®
- 0.5-0.7: å¯èƒ½æ˜¯è°£è¨€ï¼Œè¯æ®ä¸å……åˆ†
- 0.3-0.5: å¯èƒ½ä¸æ˜¯è°£è¨€
- 0.1-0.3: å¾ˆå¯èƒ½ä¸æ˜¯è°£è¨€
- 0.0-0.1: å‡ ä¹è‚¯å®šä¸æ˜¯è°£è¨€

è¯·åŸºäºåˆ†æç»™å‡ºç²¾ç¡®åˆ°4ä½å°æ•°çš„æ¦‚ç‡å€¼,ä¸è¦æ€»æ˜¯ä½¿ç”¨0.85æˆ–0.15ã€‚
"""

# ---------------------- å·¥å…·å‡½æ•° ----------------------
# 1. è·å–æ•°æ®åº“è¿æ¥
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# 2. å¯†ç åŠ å¯†/éªŒè¯
def hash_password(password: str) -> str:
    return pwd_context.hash(password)

def verify_password(plain_pwd: str, hashed_pwd: str) -> bool:
    return pwd_context.verify(plain_pwd, hashed_pwd)

# 3. ç”Ÿæˆ/éªŒè¯Token
def create_token(user_id: int) -> str:
    expire = datetime.utcnow() + timedelta(minutes=config.ACCESS_TOKEN_EXPIRE_MINUTES)
    token_data = {"sub": str(user_id), "exp": expire}
    return jwt.encode(token_data, config.SECRET_KEY, algorithm=config.ALGORITHM)

def verify_token(token: str) -> int:
    try:
        payload = jwt.decode(token, config.SECRET_KEY, algorithms=[config.ALGORITHM])
        user_id = int(payload.get("sub"))
        return user_id
    except JWTError:
        raise HTTPException(status_code=401, detail="Tokenæ— æ•ˆ/è¿‡æœŸ")

# 4. è®¡ç®—å†…å®¹å“ˆå¸Œå€¼çš„å‡½æ•°
def calculate_content_hash(content: str) -> str:
    """è®¡ç®—æ–‡æœ¬å†…å®¹çš„MD5å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡"""
    return hashlib.md5(content.encode('utf-8')).hexdigest()

# 5. æ•°æ®åº“å»é‡æŸ¥è¯¢å‡½æ•°

def find_existing_record(db: Session, content_hash: str, user_id: int) -> dict:
    """
    æ ¹æ®å†…å®¹å“ˆå¸Œå€¼å’Œç”¨æˆ·IDåœ¨æ•°æ®åº“ä¸­æŸ¥æ‰¾ç°æœ‰è®°å½•
    è¿”å›ï¼šå¦‚æœæ‰¾åˆ°è¿”å›è®°å½•æ•°æ®ï¼Œå¦åˆ™è¿”å›None
    """
    existing_record = db.query(ReasoningRecord).filter(
        ReasoningRecord.content_hash == content_hash,
        ReasoningRecord.user_id == user_id  # æ·»åŠ ç”¨æˆ·IDè¿‡æ»¤
    ).first()
    
    if existing_record:
        # æ›´æ–°ä½¿ç”¨æ¬¡æ•°å’Œæœ€åä½¿ç”¨æ—¶é—´
        existing_record.use_count += 1
        existing_record.last_used_time = datetime.now()
        db.commit()
        
        # è§£æå­˜å‚¨çš„JSONæ•°æ®
        try:
            keywords_data = json.loads(existing_record.keywords) if existing_record.keywords else []
        except:
            keywords_data = []
        
        try:
            reasoning_steps_data = json.loads(existing_record.reasoning_steps) if existing_record.reasoning_steps else []
        except:
            reasoning_steps_data = []
        
        # è·å–ç»“è®ºï¼ˆå¦‚æœå­˜å‚¨äº†çš„è¯ï¼‰
        conclusion = ""
        try:
            if existing_record.conclusion:
                conclusion = existing_record.conclusion
            else:
                # å¦‚æœæ²¡æœ‰å­˜å‚¨ç»“è®ºï¼Œæ ¹æ®è°£è¨€æ¦‚ç‡ç”Ÿæˆ
                if existing_record.rumor_prob >= 0.7:
                    conclusion = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€æ˜¯è°£è¨€ã€‘ã€‚"
                elif existing_record.rumor_prob <= 0.3:
                    conclusion = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€ä¸æ˜¯è°£è¨€ã€‘ã€‚"
                else:
                    conclusion = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯å¯èƒ½ä¸ºè°£è¨€ï¼Œå»ºè®®è¿›ä¸€æ­¥æ ¸å®ã€‚"
        except:
            conclusion = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€ç»“è®ºå¾…å®šã€‘ã€‚"
        
        return {
            "rumor_prob": round(float(existing_record.rumor_prob), 4),
            "is_ai_generated": existing_record.is_ai_generated,
            "reasoning_steps": reasoning_steps_data,
            "keywords": keywords_data,
            "from_cache": True,
            "use_count": existing_record.use_count,
            "record_id": existing_record.id,
            "is_rumor": existing_record.rumor_prob >= 0.5,
            "conclusion": conclusion
        }
    return None
# 6. æ¨¡æ‹Ÿå¤§è¯­è¨€æ¨¡å‹æ£€æµ‹
def fake_llm_detect(content: str, type: str, keywords: list):
    rumor_prob = round(random.uniform(0, 1), 4)
    is_rumor = rumor_prob >= 0.5
    conclusion = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€æ˜¯è°£è¨€ã€‘ã€‚" if is_rumor else "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€ä¸æ˜¯è°£è¨€ã€‘ã€‚"
    reasoning_steps = [
        f"è¯†åˆ«å†…å®¹ï¼š{content[:20]}...ï¼ˆç±»å‹ï¼š{type}ï¼‰",
        f"æ£€æŸ¥äº‹å®ï¼š{'ç¬¦åˆå®¢è§‚äº‹å®' if rumor_prob < 0.5 else 'ä¸ç¬¦åˆå®¢è§‚äº‹å®'}",
        f"è¯„ä¼°åˆç†æ€§ï¼š{'éè°£è¨€' if rumor_prob < 0.5 else 'è°£è¨€'}ï¼ŒAIç”Ÿæˆæ¦‚ç‡ï¼š{round(random.uniform(0, 1), 2)}",
        f"å¾—å‡ºç»“è®ºï¼š{'åˆ¤å®šä¸ºéè°£è¨€' if rumor_prob < 0.5 else 'åˆ¤å®šä¸ºè°£è¨€'}"
    ]
    
    return {
        "rumor_prob": rumor_prob,
        "is_ai_generated": random.choice([True, False]),
        "reasoning_steps": reasoning_steps,
        "from_cache": False,
        "search_used": False,
        "is_rumor": is_rumor,
        "conclusion": conclusion
    }

# 7. åˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘æœç´¢
def should_enable_web_search(content: str, keywords: list) -> bool:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œè”ç½‘æœç´¢"""
    # æ£€æŸ¥æœç´¢é…ç½®æ˜¯å¦å¯ç”¨
    if not config.SEARCH_CONFIG.get("enable", True):
        return False
    
    # æ£€æŸ¥DuckDuckGoæ˜¯å¦å¯ç”¨
    if not DUCKDUCKGO_AVAILABLE:
        return False
    
    
    if len(content) > 10:
        return True
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¯éªŒè¯çš„å£°æ˜
    verification_triggers = [
        "ç ”ç©¶è¡¨æ˜", "æ•°æ®æ˜¾ç¤º", "ä¸“å®¶ç§°", "æœ€æ–°å‘ç°", "å®éªŒè¯æ˜",
        "æ®æŠ¥é“", "å®˜æ–¹å®£å¸ƒ", "ç§‘å­¦ç ”ç©¶", "äº‹å®è¯æ˜", "è°ƒæŸ¥æ˜¾ç¤º",
        "æ®ç»Ÿè®¡", "æ ¹æ®ç ”ç©¶", "ç§‘å­¦è¯æ˜", "ä¸“å®¶å»ºè®®", "åŒ»ç”Ÿæé†’",
        "å®éªŒè¡¨æ˜", "æ•°æ®æ˜¾ç¤º", "ç§‘å­¦ç ”ç©¶", "ä¸´åºŠå®éªŒ"
    ]
    
    content_lower = content.lower()
    for trigger in verification_triggers:
        if trigger in content_lower:
            return True
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—æˆ–ç™¾åˆ†æ¯”
    if re.search(r'\d+[%ï¼…]|\d+\.\d+', content):
        return True
    
    # åŸºäºå…³é”®è¯åˆ¤æ–­
    search_keywords = {"ç ”ç©¶", "æ•°æ®", "ç»Ÿè®¡", "å®éªŒ", "æœ€æ–°", "ç§‘å­¦", "è¯æ˜", "ä¸“å®¶", "åŒ»ç”Ÿ", "æ•™æˆ", "å®éªŒ", "ç ”ç©¶", "æ•°æ®"}
    for keyword in keywords:
        if keyword in search_keywords:
            return True
    
    # ç‰¹å®šç±»å‹å†…å®¹
    if any(word in content_lower for word in ["ç–«æƒ…", "ç–«è‹—", "æ–°å† ", "ç—…æ¯’", "éš”ç¦»", "å°åŸ"]):
        return True
    if any(word in content_lower for word in ["é£Ÿå“", "åƒ", "å–", "ä¸­æ¯’", "è‡´ç™Œ", "æœ‰æ¯’"]):
        return True
    if any(word in content_lower for word in ["å¥åº·", "å…»ç”Ÿ", "æ²»ç—…", "ç–—æ•ˆ", "åæ–¹", "ç§˜æ–¹"]):
        return True
    if any(word in content_lower for word in ["ç§‘æŠ€", "å‘æ˜", "æ–°æŠ€æœ¯", "çªç ´"]):
        return True
    
    return True

# 8. æ‰§è¡ŒDuckDuckGoæœç´¢å¹¶æ ¼å¼åŒ–ç»“æœ
def perform_web_search(content: str, keywords: list) -> dict:
    """æ‰§è¡ŒDuckDuckGoæœç´¢å¹¶è¿”å›æ ¼å¼åŒ–ç»“æœ"""
    try:
        print("ğŸ” å¼€å§‹DuckDuckGoæœç´¢éªŒè¯...")
        
        # æ‰§è¡Œæœç´¢
        search_results = search_client.search_for_rumor_verification(content, keywords)
        
        if search_results.get("success") and search_results["results"]:
            print(f"âœ… DuckDuckGoæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results['results'])} ä¸ªç›¸å…³ç»“æœ")
            
            # æ ¼å¼åŒ–æœç´¢ç»“æœæ‘˜è¦
            summary_parts = []
            summary_parts.append(f"ğŸ“¡ ç½‘ç»œéªŒè¯ä¿¡æ¯ï¼ˆæ¥è‡ªDuckDuckGoæœç´¢ï¼‰ï¼š")
            summary_parts.append(f"æœç´¢æŸ¥è¯¢æ•°ï¼š{search_results['query_count']}")
            summary_parts.append(f"æ‰¾åˆ°ç»“æœæ•°ï¼š{search_results['total_results']}")
            summary_parts.append("")
            
            for i, result in enumerate(search_results["results"][:4], 1):
                result_type_emoji = {
                    "è¾Ÿè°£ä¿¡æ¯": "ğŸš«",
                    "è¯å®ä¿¡æ¯": "âœ…", 
                    "ä¸ç¡®å®šä¿¡æ¯": "â“",
                    "æ™®é€šä¿¡æ¯": "ğŸ“°"
                }.get(result["type"], "ğŸ“°")
                
                summary_parts.append(f"{i}. {result_type_emoji} {result['title']}")
                summary_parts.append(f"   æ‘˜è¦ï¼š{result['summary']}")
                summary_parts.append(f"   ç±»å‹ï¼š{result['type']} | æ¥æºï¼š{result['source']}")
                summary_parts.append("")
            
            return {
                "success": True,
                "summary": "\n".join(summary_parts),
                "raw_results": search_results["results"],
                "formatted_results": search_results["results"][:4]
            }
        else:
            print("â„¹ï¸ æœªè·å–åˆ°æœ‰æ•ˆçš„ç½‘ç»œéªŒè¯ä¿¡æ¯")
            return {
                "success": False,
                "summary": "âš ï¸ ç½‘ç»œæœç´¢æœªæ‰¾åˆ°ç›¸å…³éªŒè¯ä¿¡æ¯",
                "raw_results": [],
                "formatted_results": []
            }
            
    except Exception as e:
        print(f"âŒ DuckDuckGoæœç´¢å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "summary": f"âŒ ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}",
            "raw_results": [],
            "formatted_results": []
        }

# 9. å¢å¼ºçš„æ£€æµ‹å‡½æ•°ï¼ˆå¸¦DuckDuckGoæœç´¢ï¼‰
def enhanced_real_llm_detect(content: str, type: str, keywords: list, search_enabled: bool = True):
    """å¢å¼ºçš„æ£€æµ‹å‡½æ•°ï¼ŒåŒ…å«DuckDuckGoæœç´¢"""
    try:
        print(f"ğŸ“ è°ƒç”¨GLM-4æ¨¡å‹APIï¼Œå†…å®¹é•¿åº¦: {len(content)}")
        print(f"ğŸ” ç”¨æˆ·æœç´¢è®¾ç½®: {'å¯ç”¨' if search_enabled else 'ç¦ç”¨'}")
        
        fact_check_result = FactChecker.check_simple_facts(content)
        print(f"ğŸ” äº‹å®æ£€æŸ¥ç»“æœ: {fact_check_result}")
        
        # å¦‚æœäº‹å®æ£€æŸ¥æœ‰ç¡®å®šç»“æœï¼Œä¼˜å…ˆä½¿ç”¨
        if fact_check_result["certainty"] > 0.9 and fact_check_result["is_factual"] is not None:
            print(f"âœ… ä½¿ç”¨äº‹å®æ£€æŸ¥ç»“æœï¼Œç¡®å®šæ€§: {fact_check_result['certainty']}")
            
            is_rumor = not fact_check_result["is_factual"]
            rumor_prob = 0.9 if is_rumor else 0.1  # äº‹å®é”™è¯¯å°±æ˜¯é«˜æ¦‚ç‡è°£è¨€
            
            reasoning_steps = [
                f"ç¬¬ä¸€æ­¥ï¼šè¯†åˆ«å†…å®¹ä¸­çš„äº‹å®é™ˆè¿°",
                f"ç¬¬äºŒæ­¥ï¼šæŸ¥è¯¢å®¢è§‚äº‹å®ï¼ˆ{fact_check_result['fact_type']}ï¼‰",
                f"ç¬¬ä¸‰æ­¥ï¼šå¯¹æ¯”äº‹å®ï¼š{fact_check_result['correction']}",
                f"ç¬¬å››æ­¥ï¼šåŸºäºå®¢è§‚äº‹å®å¾—å‡ºç»“è®º"
            ]
            
            conclusion = f"ç»å®¢è§‚äº‹å®æ ¸æŸ¥ï¼Œè¯¥ä¿¡æ¯{'ã€æ˜¯è°£è¨€ã€‘' if is_rumor else 'ã€ä¸æ˜¯è°£è¨€ã€‘'}ã€‚{fact_check_result['correction']}"
            
            return {
                "reasoning_steps": reasoning_steps,
                "is_ai_generated": False,
                "rumor_prob": rumor_prob,
                "is_rumor": is_rumor,
                "conclusion": conclusion,
                "from_cache": False,
                "web_context_used": False,
                "search_used": False,
                "search_result_count": 0,
                "confidence": "é«˜",
                "verification_based_on_search": False,
                "fact_check_used": True,
                "fact_check_result": fact_check_result
            }
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘æœç´¢ - è€ƒè™‘ç”¨æˆ·è®¾ç½®
        should_search = False
        if search_enabled and config.SEARCH_CONFIG.get("user_can_disable", True):
            should_search = should_enable_web_search(content, keywords)
        
        web_context = {"success": False, "summary": "", "results": []}
        
        # å¦‚æœéœ€è¦æœç´¢ï¼Œæ‰§è¡ŒDuckDuckGoæœç´¢
        if should_search:
            web_context = perform_web_search(content, keywords)
            print(f"ğŸ“¡ ç½‘ç»œéªŒè¯: {'æˆåŠŸ' if web_context['success'] else 'å¤±è´¥æˆ–æ— ç»“æœ'}")
        else:
            print(f"ğŸ”‡ æœç´¢åŠŸèƒ½è¢«{'ç”¨æˆ·ç¦ç”¨' if not search_enabled else 'ç³»ç»Ÿåˆ¤æ–­ä¸éœ€è¦'}")
        
        # æ„å»ºæç¤ºè¯
        escaped_content = content.replace("{", "{{").replace("}", "}}")
        escaped_type = type.replace("{", "{{").replace("}", "}}")
        escaped_keywords = str(keywords).replace("{", "{{").replace("}", "}}")
        
        # é€‰æ‹©æç¤ºè¯æ¨¡æ¿
        if web_context.get("success") and web_context.get("summary"):
            # ä½¿ç”¨å¢å¼ºæ¨¡æ¿
            prompt_content = ENHANCED_PROMPT_TEMPLATE.format(
                content=escaped_content,
                type=escaped_type,
                keywords=escaped_keywords,
                search_summary=web_context["summary"]
            )
        else:
            # ä½¿ç”¨åŸå§‹æ¨¡æ¿
            prompt_content = PROMPT_TEMPLATE.format(
                content=escaped_content,
                type=escaped_type,
                keywords=escaped_keywords
            )
        
        # è°ƒç”¨GLM-4
        print(f"ğŸ“¤ å‘é€æç¤ºè¯ç»™GLM-4ï¼ˆå‰100å­—ç¬¦ï¼‰: {prompt_content[:100]}...")
        response = llm_client.chat.completions.create(
            model=config.LLM_CONFIG["model_name"],
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„è°£è¨€ç”„åˆ«ä¸“å®¶ï¼Œè¯·åŸºäºæ‰€æœ‰å¯ç”¨ä¿¡æ¯è¿›è¡Œå®¢è§‚åˆ†æï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºã€‚"},
                {"role": "user", "content": prompt_content}
            ],
            temperature=config.LLM_CONFIG["temperature"],
            max_tokens=config.LLM_CONFIG["max_tokens"],
            timeout=30
        )
        
        if not response or not response.choices:
            print("âŒ APIè¿”å›ç©ºå“åº”")
            # å›é€€åˆ°åŸå§‹æ£€æµ‹
            return real_llm_detect(content, type, keywords)
        
        result_str = response.choices[0].message.content.strip()
        
        if not result_str or len(result_str) == 0:
            print("âŒ æ¨¡å‹è¿”å›ç©ºå†…å®¹")
            # å›é€€åˆ°åŸå§‹æ£€æµ‹
            return real_llm_detect(content, type, keywords)
        
        # åœ¨è¿™é‡Œæ·»åŠ å…³é”®æ—¥å¿—ï¼Œæ‰“å°åŸå§‹å“åº”
        print(f"ğŸ¤– åŸå§‹APIå“åº”ï¼ˆå‰500å­—ç¬¦ï¼‰: {result_str[:500]}...")
        print(f"ğŸ¤– åŸå§‹APIå“åº”å®Œæ•´é•¿åº¦: {len(result_str)} å­—ç¬¦")
        
        # æ¸…ç†å“åº”æ–‡æœ¬
        if result_str.startswith("```json"):
            result_str = result_str.replace("```json", "").replace("```", "").strip()
        elif result_str.startswith("```"):
            result_str = result_str.replace("```", "").strip()
        
        result_str = re.sub(r'<[^>]+>', '', result_str)
        result_str = re.sub(r'^JSON:\s*', '', result_str, flags=re.IGNORECASE)
        result_str = result_str.strip()
        
        print(f"ğŸ”„ æ¸…ç†åçš„å“åº”: {result_str[:200]}...")
        
        # JSONè§£æ
        result = None
        max_attempts = 3
        for attempt in range(max_attempts):
            try:
                result = json.loads(result_str)
                print(f"âœ… JSONè§£ææˆåŠŸï¼ˆç¬¬{attempt+1}æ¬¡å°è¯•ï¼‰")
                break
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSONè§£æå¤±è´¥ï¼ˆç¬¬{attempt+1}æ¬¡å°è¯•ï¼‰: {str(e)}")
                if attempt < max_attempts - 1:
                    # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                    if "'" in result_str:
                        result_str = result_str.replace("'", "\"")
                    result_str = re.sub(r'\s+', ' ', result_str)
                    
                    # ç¡®ä¿æœ‰å®Œæ•´çš„JSONç»“æ„
                    if not result_str.endswith("}"):
                        # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ª}
                        last_brace = result_str.rfind("}")
                        if last_brace != -1:
                            result_str = result_str[:last_brace+1]
                        else:
                            result_str += "}"
                    
                    if not result_str.startswith("{"):
                        start_idx = result_str.find("{")
                        if start_idx != -1:
                            result_str = result_str[start_idx:]
                        else:
                            result_str = "{" + result_str
                    
                    print(f"ğŸ”„ ä¿®å¤åé‡æ–°å°è¯•: {result_str[:100]}...")
                else:
                    print("âŒ æ‰€æœ‰JSONè§£æå°è¯•éƒ½å¤±è´¥")
                    raise Exception(f"JSONè§£æå¤±è´¥: {str(e)}")
        
        if result is None:
            raise Exception("JSONè§£æå¤±è´¥")
        
        # è¡¥å…¨å­—æ®µ
        required_fields = ["reasoning_steps", "is_ai_generated", "rumor_prob", "is_rumor", "conclusion"]
        for field in required_fields:
            if field not in result:
                print(f"âš ï¸ ç¼ºå¤±å­—æ®µ: {field}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                if field == "reasoning_steps":
                    result[field] = ["è¯†åˆ«å†…å®¹ï¼šä¿¡æ¯ä¸è¶³", "æ£€æŸ¥äº‹å®ï¼šæ— ç›¸å…³ä¾æ®", "è¯„ä¼°åˆç†æ€§ï¼šæ— æ³•åˆ¤æ–­", "å¾—å‡ºç»“è®ºï¼šä¿¡æ¯ä¸è¶³"]
                elif field == "is_ai_generated":
                    result[field] = False
                elif field == "rumor_prob":
                    result[field] = 0.5000
                elif field == "is_rumor":
                    result[field] = result.get("rumor_prob", 0.5) >= 0.5
                elif field == "conclusion":
                    is_rumor = result.get("is_rumor", result.get("rumor_prob", 0.5) >= 0.5)
                    result[field] = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€æ˜¯è°£è¨€ã€‘ã€‚" if is_rumor else "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€ä¸æ˜¯è°£è¨€ã€‘ã€‚"
        
        # ç¡®ä¿æ¨ç†æ­¥éª¤æ˜¯4æ­¥
        if "reasoning_steps" in result:
            if not isinstance(result["reasoning_steps"], list):
                result["reasoning_steps"] = ["è¯†åˆ«å†…å®¹ï¼šä¿¡æ¯ä¸è¶³", "æ£€æŸ¥äº‹å®ï¼šæ— ç›¸å…³ä¾æ®", "è¯„ä¼°åˆç†æ€§ï¼šæ— æ³•åˆ¤æ–­", "å¾—å‡ºç»“è®ºï¼šä¿¡æ¯ä¸è¶³"]
            elif len(result["reasoning_steps"]) != 4:
                print(f"âš ï¸ æ¨ç†æ­¥éª¤æ•°é‡ä¸æ­£ç¡®: {len(result['reasoning_steps'])}ï¼Œè°ƒæ•´ä¸º4æ­¥")
                if len(result["reasoning_steps"]) < 4:
                    while len(result["reasoning_steps"]) < 4:
                        result["reasoning_steps"].append("ä¿¡æ¯ä¸è¶³")
                result["reasoning_steps"] = result["reasoning_steps"][:4]
        
        # ç¡®ä¿è°£è¨€æ¦‚ç‡æ ¼å¼æ­£ç¡®
        if "rumor_prob" in result:
            try:
                rumor_prob = float(result["rumor_prob"])
                rumor_prob = max(0.0, min(1.0, rumor_prob))
                result["rumor_prob"] = round(rumor_prob, 4)
                print(f"âœ… è°£è¨€æ¦‚ç‡å¤„ç†æˆåŠŸ: {result['rumor_prob']}")
            except Exception as e:
                print(f"âš ï¸ è°£è¨€æ¦‚ç‡æ ¼å¼é”™è¯¯: {result['rumor_prob']}ï¼Œä½¿ç”¨é»˜è®¤å€¼0.5")
                result["rumor_prob"] = 0.5000
        
        # ç¡®ä¿ç»“è®ºå­—æ®µæ ¼å¼æ­£ç¡®
        if "conclusion" not in result or not result["conclusion"]:
            is_rumor = result.get("is_rumor", result.get("rumor_prob", 0.5) >= 0.5)
            result["conclusion"] = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€æ˜¯è°£è¨€ã€‘ã€‚" if is_rumor else "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€ä¸æ˜¯è°£è¨€ã€‘ã€‚"
        
        # ç¡®ä¿ç»“è®ºåŒ…å«æ˜ç¡®åˆ¤æ–­
        if "ã€æ˜¯è°£è¨€ã€‘" not in result["conclusion"] and "ã€ä¸æ˜¯è°£è¨€ã€‘" not in result["conclusion"]:
            is_rumor = result.get("is_rumor", result.get("rumor_prob", 0.5) >= 0.5)
            result["conclusion"] = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€æ˜¯è°£è¨€ã€‘ã€‚" if is_rumor else "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€ä¸æ˜¯è°£è¨€ã€‘ã€‚"
        
        # ç¡®ä¿is_rumorå­—æ®µä¸ç»“è®ºä¸€è‡´
        if "is_rumor" not in result:
            result["is_rumor"] = "ã€æ˜¯è°£è¨€ã€‘" in result["conclusion"]
        
        # æ·»åŠ é¢å¤–å­—æ®µ
        result["from_cache"] = False
        result["web_context_used"] = web_context.get("success", False)
        
        if web_context.get("success"):
            result["search_used"] = True
            result["search_result_count"] = len(web_context.get("raw_results", []))
            result["search_summary"] = web_context.get("summary", "")[:300] + "..." if len(web_context.get("summary", "")) > 300 else web_context.get("summary", "")
            
            # ç¡®ä¿å¢å¼ºæ¨¡æ¿çš„å­—æ®µå­˜åœ¨
            if "verification_based_on_search" not in result:
                result["verification_based_on_search"] = True
            if "search_result_summary" not in result:
                result["search_result_summary"] = "åŸºäºæœç´¢ç»“æœè¿›è¡Œäº†äº‹å®æ ¸æŸ¥"
            if "key_findings_from_search" not in result:
                result["key_findings_from_search"] = ["æœç´¢ç»“æœä¸­åŒ…å«ç›¸å…³ä¿¡æ¯"]
        else:
            result["search_used"] = False
            result["search_result_count"] = 0
        
        # ç¡®ä¿æœ‰confidenceå­—æ®µ
        if "confidence" not in result:
            result["confidence"] = "ä¸­"
        
        # ç¡®ä¿æœ‰verification_suggestionså­—æ®µ
        if "verification_suggestions" not in result:
            result["verification_suggestions"] = ["å»ºè®®è¿›ä¸€æ­¥æ ¸å®ä¿¡æ¯æ¥æº"]
        
        print(f"âœ… å¢å¼ºæ£€æµ‹å®Œæˆï¼Œè°£è¨€æ¦‚ç‡: {result['rumor_prob']}, ç»“è®º: {result['conclusion']}")
        return result
        
    except Exception as e:
        print(f"âŒ å¢å¼ºæ£€æµ‹å¤±è´¥ï¼š{str(e)}")
        print(f"âŒ é”™è¯¯ç±»å‹ï¼š{type(e).__name__}")
        print(f"âŒ è¯¦ç»†é”™è¯¯ï¼š{traceback.format_exc()}")
        # å›é€€åˆ°åŸå§‹æ£€æµ‹
        return real_llm_detect(content, type, keywords)

# 10. åŸå§‹æ£€æµ‹å‡½æ•° 
def real_llm_detect(content: str, type: str, keywords: list):
    """åŸå§‹çš„å¤§è¯­è¨€æ¨¡å‹æ£€æµ‹å‡½æ•°"""
    try:
        print(f"ğŸ“ è°ƒç”¨GLM-4æ¨¡å‹APIï¼ˆåŸå§‹æ¨¡å¼ï¼‰ï¼Œå†…å®¹é•¿åº¦: {len(content)}")
        
        escaped_content = content.replace("{", "{{").replace("}", "}}")
        escaped_type = type.replace("{", "{{").replace("}", "}}")
        escaped_keywords = str(keywords).replace("{", "{{").replace("}", "}}")
        
        prompt_content = PROMPT_TEMPLATE.format(
            content=escaped_content, 
            type=escaped_type,
            keywords=escaped_keywords
        )
        
        print(f"ğŸ“¤ å‘é€æç¤ºè¯ç»™GLM-4ï¼ˆåŸå§‹æ¨¡å¼ï¼Œå‰100å­—ç¬¦ï¼‰: {prompt_content[:100]}...")
        
        try:
            response = llm_client.chat.completions.create(
                model=config.LLM_CONFIG["model_name"],
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è°£è¨€ç”„åˆ«ä¸“å®¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºJSONæ ¼å¼çš„ç»“æœï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"},
                    {"role": "user", "content": prompt_content}
                ],
                temperature=config.LLM_CONFIG["temperature"],
                max_tokens=config.LLM_CONFIG["max_tokens"],
                timeout=30
            )
        except Exception as api_error:
            print(f"âŒ ç¬¬ä¸€æ¬¡APIè°ƒç”¨å¤±è´¥: {str(api_error)}")
            try:
                response = llm_client.chat.completions.create(
                    model=config.LLM_CONFIG["model_name"],
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è°£è¨€ç”„åˆ«ä¸“å®¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºJSONæ ¼å¼çš„ç»“æœï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"},
                        {"role": "user", "content": prompt_content}
                    ],
                    temperature=config.LLM_CONFIG["temperature"],
                    max_tokens=config.LLM_CONFIG["max_tokens"],
                    timeout=30
                )
            except Exception as retry_error:
                print(f"âŒ ç¬¬äºŒæ¬¡APIè°ƒç”¨å¤±è´¥: {str(retry_error)}")
                raise Exception(f"APIè°ƒç”¨å¤±è´¥: {str(retry_error)}")
        
        if not response or not response.choices:
            print("âŒ APIè¿”å›ç©ºå“åº”")
            raise Exception("APIè¿”å›ç©ºå“åº”")
        
        result_str = response.choices[0].message.content.strip()
        
        if not result_str or len(result_str) == 0:
            print("âŒ æ¨¡å‹è¿”å›ç©ºå†…å®¹")
            raise Exception("æ¨¡å‹è¿”å›ç©ºå†…å®¹")
        
        # åœ¨è¿™é‡Œæ·»åŠ å…³é”®æ—¥å¿—ï¼Œæ‰“å°åŸå§‹å“åº”
        print(f"ğŸ¤– åŸå§‹APIå“åº”ï¼ˆåŸå§‹æ¨¡å¼ï¼Œå‰500å­—ç¬¦ï¼‰: {result_str[:500]}...")
        print(f"ğŸ¤– åŸå§‹APIå“åº”å®Œæ•´é•¿åº¦: {len(result_str)} å­—ç¬¦")
        
        # æ¸…ç†å“åº”æ–‡æœ¬
        if result_str.startswith("```json"):
            result_str = result_str.replace("```json", "").replace("```", "").strip()
        elif result_str.startswith("```"):
            result_str = result_str.replace("```", "").strip()
        
        result_str = re.sub(r'<[^>]+>', '', result_str)
        result_str = re.sub(r'^JSON:\s*', '', result_str, flags=re.IGNORECASE)
        result_str = result_str.strip()
        
        print(f"ğŸ”„ æ¸…ç†åçš„å“åº”ï¼ˆåŸå§‹æ¨¡å¼ï¼‰: {result_str[:200]}...")
        
        # JSONè§£æ
        result = None
        max_attempts = 3
        for attempt in range(max_attempts):
            try:
                result = json.loads(result_str)
                print(f"âœ… JSONè§£ææˆåŠŸï¼ˆåŸå§‹æ¨¡å¼ï¼Œç¬¬{attempt+1}æ¬¡å°è¯•ï¼‰")
                break
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSONè§£æå¤±è´¥ï¼ˆåŸå§‹æ¨¡å¼ï¼Œç¬¬{attempt+1}æ¬¡å°è¯•ï¼‰: {str(e)}")
                if attempt < max_attempts - 1:
                    if "'" in result_str:
                        result_str = result_str.replace("'", "\"")
                    result_str = re.sub(r'\s+', ' ', result_str)
                    
                    # ç¡®ä¿æœ‰å®Œæ•´çš„JSONç»“æ„
                    if not result_str.endswith("}"):
                        last_brace = result_str.rfind("}")
                        if last_brace != -1:
                            result_str = result_str[:last_brace+1]
                        else:
                            result_str += "}"
                    
                    if not result_str.startswith("{"):
                        start_idx = result_str.find("{")
                        if start_idx != -1:
                            result_str = result_str[start_idx:]
                        else:
                            result_str = "{" + result_str
                    
                    print(f"ğŸ”„ ä¿®å¤åé‡æ–°å°è¯•ï¼ˆåŸå§‹æ¨¡å¼ï¼‰: {result_str[:100]}...")
                else:
                    print("âŒ æ‰€æœ‰JSONè§£æå°è¯•éƒ½å¤±è´¥ï¼ˆåŸå§‹æ¨¡å¼ï¼‰")
                    raise Exception(f"JSONè§£æå¤±è´¥: {str(e)}")
        
        if result is None:
            raise Exception("JSONè§£æå¤±è´¥")
        
        # è¡¥å…¨å­—æ®µ
        required_fields = ["reasoning_steps", "is_ai_generated", "rumor_prob", "is_rumor", "conclusion"]
        for field in required_fields:
            if field not in result:
                print(f"âš ï¸ ç¼ºå¤±å­—æ®µï¼ˆåŸå§‹æ¨¡å¼ï¼‰: {field}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                if field == "reasoning_steps":
                    result[field] = ["è¯†åˆ«å†…å®¹ï¼šä¿¡æ¯ä¸è¶³", "æ£€æŸ¥äº‹å®ï¼šæ— ç›¸å…³ä¾æ®", "è¯„ä¼°åˆç†æ€§ï¼šæ— æ³•åˆ¤æ–­", "å¾—å‡ºç»“è®ºï¼šä¿¡æ¯ä¸è¶³"]
                elif field == "is_ai_generated":
                    result[field] = False
                elif field == "rumor_prob":
                    result[field] = 0.5000
                elif field == "is_rumor":
                    result[field] = result.get("rumor_prob", 0.5) >= 0.5
                elif field == "conclusion":
                    is_rumor = result.get("is_rumor", result.get("rumor_prob", 0.5) >= 0.5)
                    result[field] = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€æ˜¯è°£è¨€ã€‘ã€‚" if is_rumor else "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€ä¸æ˜¯è°£è¨€ã€‘ã€‚"
        
        # ç¡®ä¿æ¨ç†æ­¥éª¤æ˜¯4æ­¥
        if "reasoning_steps" in result:
            if not isinstance(result["reasoning_steps"], list):
                result["reasoning_steps"] = ["è¯†åˆ«å†…å®¹ï¼šä¿¡æ¯ä¸è¶³", "æ£€æŸ¥äº‹å®ï¼šæ— ç›¸å…³ä¾æ®", "è¯„ä¼°åˆç†æ€§ï¼šæ— æ³•åˆ¤æ–­", "å¾—å‡ºç»“è®ºï¼šä¿¡æ¯ä¸è¶³"]
            elif len(result["reasoning_steps"]) != 4:
                print(f"âš ï¸ æ¨ç†æ­¥éª¤æ•°é‡ä¸æ­£ç¡®ï¼ˆåŸå§‹æ¨¡å¼ï¼‰: {len(result['reasoning_steps'])}ï¼Œè°ƒæ•´ä¸º4æ­¥")
                if len(result["reasoning_steps"]) < 4:
                    while len(result["reasoning_steps"]) < 4:
                        result["reasoning_steps"].append("ä¿¡æ¯ä¸è¶³")
                result["reasoning_steps"] = result["reasoning_steps"][:4]
        
        # ç¡®ä¿è°£è¨€æ¦‚ç‡æ ¼å¼æ­£ç¡®
        if "rumor_prob" in result:
            try:
                rumor_prob = float(result["rumor_prob"])
                rumor_prob = max(0.0, min(1.0, rumor_prob))
                result["rumor_prob"] = round(rumor_prob, 4)
                print(f"âœ… è°£è¨€æ¦‚ç‡å¤„ç†æˆåŠŸï¼ˆåŸå§‹æ¨¡å¼ï¼‰: {result['rumor_prob']}")
            except Exception as e:
                print(f"âš ï¸ è°£è¨€æ¦‚ç‡æ ¼å¼é”™è¯¯ï¼ˆåŸå§‹æ¨¡å¼ï¼‰: {result['rumor_prob']}ï¼Œä½¿ç”¨é»˜è®¤å€¼0.5")
                result["rumor_prob"] = 0.5000
        
        # ç¡®ä¿ç»“è®ºå­—æ®µæ ¼å¼æ­£ç¡®
        if "conclusion" not in result or not result["conclusion"]:
            is_rumor = result.get("is_rumor", result.get("rumor_prob", 0.5) >= 0.5)
            result["conclusion"] = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€æ˜¯è°£è¨€ã€‘ã€‚" if is_rumor else "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€ä¸æ˜¯è°£è¨€ã€‘ã€‚"
        
        # ç¡®ä¿ç»“è®ºåŒ…å«æ˜ç¡®åˆ¤æ–­
        if "ã€æ˜¯è°£è¨€ã€‘" not in result["conclusion"] and "ã€ä¸æ˜¯è°£è¨€ã€‘" not in result["conclusion"]:
            is_rumor = result.get("is_rumor", result.get("rumor_prob", 0.5) >= 0.5)
            result["conclusion"] = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€æ˜¯è°£è¨€ã€‘ã€‚" if is_rumor else "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€ä¸æ˜¯è°£è¨€ã€‘ã€‚"
        
        # ç¡®ä¿is_rumorå­—æ®µä¸ç»“è®ºä¸€è‡´
        if "is_rumor" not in result:
            result["is_rumor"] = "ã€æ˜¯è°£è¨€ã€‘" in result["conclusion"]
        
        result["from_cache"] = False
        result["web_context_used"] = False
        result["search_used"] = False
        result["search_result_count"] = 0
        
        print(f"âœ… åŸå§‹æ£€æµ‹å®Œæˆï¼Œè°£è¨€æ¦‚ç‡: {result['rumor_prob']}, ç»“è®º: {result['conclusion']}")
        return result
        
    except Exception as e:
        print(f"âŒ GLM-4æ¨¡å‹è°ƒç”¨/è§£æå¤±è´¥ï¼š{str(e)}")
        print(f"âŒ é”™è¯¯ç±»å‹ï¼š{type(e).__name__}")
        print(f"âŒ è¯¦ç»†é”™è¯¯ï¼š{traceback.format_exc()}")
        return {
            "reasoning_steps": ["è¯†åˆ«å†…å®¹ï¼šæ¨¡å‹è°ƒç”¨å¼‚å¸¸", "æ£€æŸ¥äº‹å®ï¼šæ£€æµ‹å¤±è´¥", "è¯„ä¼°åˆç†æ€§ï¼šæ— æ³•åˆ¤æ–­", "å¾—å‡ºç»“è®ºï¼šä¿¡æ¯ä¸è¶³"],
            "is_ai_generated": False,
            "rumor_prob": 0.5000,
            "is_rumor": False,
            "conclusion": "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€æ£€æµ‹å¤±è´¥ï¼Œè¯·é‡è¯•ã€‘ã€‚",
            "from_cache": False,
            "web_context_used": False,
            "search_used": False,
            "search_result_count": 0
        }

# ---------------------- æ•°æ®æ¨¡å‹ ----------------------
class RegisterRequest(BaseModel):
    username: str
    password: str
    confirm_password: str

class LoginRequest(BaseModel):
    username: str
    password: str

class DetectRequest(BaseModel):
    content: str
    type: str
    search_enabled: bool = True  # æ–°å¢ï¼šç”¨æˆ·æ˜¯å¦å¯ç”¨æœç´¢

# ---------------------- æ ¸å¿ƒæ¥å£ ----------------------
@app.post("/api/register")
def register(request: RegisterRequest, db: Session = Depends(get_db)):
    if not request.username or not request.password or not request.confirm_password:
        raise HTTPException(status_code=400, detail="æ‰€æœ‰å­—æ®µä¸èƒ½ä¸ºç©º")
    
    if len(request.password) < 6 or len(request.password) > 72:
        raise HTTPException(status_code=400, detail="å¯†ç é•¿åº¦éœ€6-72ä½")
    
    if request.password != request.confirm_password:
        raise HTTPException(status_code=400, detail="ä¸¤æ¬¡è¾“å…¥çš„å¯†ç ä¸ä¸€è‡´")
    
    if db.query(User).filter(User.username == request.username).first():
        raise HTTPException(status_code=400, detail="ç”¨æˆ·åå·²å­˜åœ¨")
    
    hashed_pwd = hash_password(request.password[:72])
    new_user = User(
        username=request.username,
        password=hashed_pwd,
        create_time=datetime.now()
    )
    try:
        db.add(new_user)
        db.commit()
        db.refresh(new_user)
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºç”¨æˆ·å¤±è´¥ï¼š{str(e)}")
    
    return {
        "code": 200,
        "msg": "æ³¨å†ŒæˆåŠŸï¼Œè¯·ç™»å½•",
        "data": {
            "user_id": new_user.id,
            "username": new_user.username
        }
    }

@app.post("/api/login")
def login(request: LoginRequest, req: Request, db: Session = Depends(get_db)):
    if not request.username or not request.password:
        raise HTTPException(status_code=400, detail="ç”¨æˆ·å/å¯†ç ä¸èƒ½ä¸ºç©º")
    
    user = db.query(User).filter(User.username == request.username).first()
    if not user:
        raise HTTPException(status_code=401, detail="ç”¨æˆ·åä¸å­˜åœ¨")
    
    if not verify_password(request.password, user.password):
        raise HTTPException(status_code=401, detail="å¯†ç é”™è¯¯")
    
    try:
        login_log = LoginLog(user_id=user.id, ip=req.client.host)
        db.add(login_log)
        db.commit()
    except Exception as e:
        db.rollback()
        print(f"è®°å½•ç™»å½•æ—¥å¿—å¤±è´¥ï¼š{str(e)}")
    
    token = create_token(user.id)
    return {
        "code": 200,
        "msg": "ç™»å½•æˆåŠŸ",
        "data": {
            "token": token,
            "user_id": user.id,
            "username": user.username
        }
    }

# ---------------------- æ£€æµ‹æ¥å£ ----------------------
@app.post("/api/detect")
def detect(
    request: DetectRequest,
    authorization: str = Header(None),
    db: Session = Depends(get_db)
):
    # 1. éªŒè¯Token
    if not authorization or not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="è¯·å…ˆç™»å½•")
    token = authorization.split(" ")[1]
    try:
        user_id = verify_token(token)
    except:
        raise HTTPException(status_code=401, detail="Tokenæ— æ•ˆ/è¿‡æœŸ")
    
    # 2. æ ¡éªŒæ–‡æœ¬é•¿åº¦
    if len(request.content) < 1 or len(request.content) > 500:
        raise HTTPException(status_code=400, detail="æ–‡æœ¬é•¿åº¦éœ€1-500å­—")
    
    # 3. è®¡ç®—å†…å®¹å“ˆå¸Œå€¼
    content_hash = calculate_content_hash(request.content)
    print(f"ğŸ”‘ å†…å®¹å“ˆå¸Œå€¼: {content_hash}, ç”¨æˆ·ID: {user_id}")
    
    # 4. å…ˆæŸ¥è¯¢æ•°æ®åº“æ˜¯å¦æœ‰ç›¸åŒå†…å®¹çš„è®°å½•
    existing_record = find_existing_record(db, content_hash, user_id)  # ä¼ å…¥user_idå‚æ•°
    if existing_record:
        print(f"âœ… æ‰¾åˆ°ç”¨æˆ·{user_id}çš„ç¼“å­˜è®°å½•ï¼Œä½¿ç”¨æ¬¡æ•°: {existing_record['use_count']}")
        return {
            "code": 200,
            "msg": "æ£€æµ‹æˆåŠŸï¼ˆæ¥è‡ªç¼“å­˜ï¼‰",
            "data": {
                "rumor_prob": existing_record["rumor_prob"],
                "is_ai_generated": existing_record["is_ai_generated"],
                "reasoning_steps": existing_record["reasoning_steps"],
                "keywords": existing_record["keywords"],
                "record_id": existing_record["record_id"],
                "from_cache": True,
                "use_count": existing_record["use_count"],
                "web_context_used": False,
                "search_used": False,
                "is_rumor": existing_record["is_rumor"],
                "conclusion": existing_record["conclusion"]
            }
        }
    
    # 5. å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œåˆ™æå–å…³é”®å­—
    keywords = extract_keywords_with_jieba(request.content)
    print(f"ğŸ”‘ æå–çš„å…³é”®å­—: {keywords}")
    print(f"ğŸ”„ ç”¨æˆ·{user_id}æœªæ‰¾åˆ°ç¼“å­˜è®°å½•ï¼Œè°ƒç”¨å¤§æ¨¡å‹...")
    # 6. è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹
    if config.LLM_FAKE:
        print("ğŸ¤– ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
        llm_result = fake_llm_detect(request.content, request.type, keywords)
    else:
        print(f"ğŸš€ ä½¿ç”¨GLM-4çœŸå®APIæ¨¡å¼ï¼Œæœç´¢åŠŸèƒ½: {'å¯ç”¨' if request.search_enabled else 'ç¦ç”¨'}")
        llm_result = enhanced_real_llm_detect(request.content, request.type, keywords, request.search_enabled)
    
    # 7. å­˜å‚¨æ–°çš„æ£€æµ‹è®°å½•åˆ°æ•°æ®åº“
    try:
        record = ReasoningRecord(
            user_id=user_id,
            content=request.content,
            content_hash=content_hash,
            type=request.type,
            rumor_prob=llm_result["rumor_prob"],
            is_ai_generated=llm_result["is_ai_generated"],
            reasoning_steps=json.dumps(llm_result["reasoning_steps"], ensure_ascii=False),
            keywords=json.dumps(keywords, ensure_ascii=False),
            use_count=1,
            create_time=datetime.now(),
            last_used_time=datetime.now(),
            conclusion=llm_result.get("conclusion", "")
        )
        db.add(record)
        db.commit()
        db.refresh(record)
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"å­˜å‚¨æ£€æµ‹è®°å½•å¤±è´¥ï¼š{str(e)}")
    
    # 8. è¿”å›ç»“æœç»™å‰ç«¯
    response_data = {
        "rumor_prob": round(llm_result["rumor_prob"], 4),
        "is_ai_generated": llm_result["is_ai_generated"],
        "reasoning_steps": llm_result["reasoning_steps"],
        "keywords": keywords,
        "record_id": record.id,
        "from_cache": False,
        "use_count": 1,
        "web_context_used": llm_result.get("web_context_used", False),
        "search_used": llm_result.get("search_used", False),
        "is_rumor": llm_result.get("is_rumor", llm_result["rumor_prob"] >= 0.5),
        "conclusion": llm_result.get("conclusion", "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€ç»“è®ºå¾…å®šã€‘ã€‚")
    }
    
    # æ·»åŠ é¢å¤–å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if "confidence" in llm_result:
        response_data["confidence"] = llm_result["confidence"]
    if "verification_suggestions" in llm_result:
        response_data["verification_suggestions"] = llm_result["verification_suggestions"]
    if "search_summary" in llm_result:
        response_data["search_summary"] = llm_result["search_summary"]
    if "search_result_count" in llm_result:
        response_data["search_result_count"] = llm_result["search_result_count"]
    if "verification_based_on_search" in llm_result:
        response_data["verification_based_on_search"] = llm_result["verification_based_on_search"]
    if "search_result_summary" in llm_result:
        response_data["search_result_summary"] = llm_result["search_result_summary"]
    if "key_findings_from_search" in llm_result:
        response_data["key_findings_from_search"] = llm_result["key_findings_from_search"]
    
    return {
        "code": 200,
        "msg": "æ£€æµ‹æˆåŠŸ" + ("ï¼ˆå«ç½‘ç»œéªŒè¯ï¼‰" if llm_result.get("search_used") else "ï¼ˆå®æ—¶åˆ†æï¼‰"),
        "data": response_data
    }

@app.get("/api/history")
def get_history(
    authorization: str = Header(None),
    page: int = 1,
    size: int = 10,
    db: Session = Depends(get_db)
):
    if not authorization or not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="è¯·å…ˆç™»å½•")
    token = authorization.split(" ")[1]
    try:
        user_id = verify_token(token)
    except:
        raise HTTPException(status_code=401, detail="Tokenæ— æ•ˆ/è¿‡æœŸ")
    
    if page < 1:
        page = 1
    if size < 1 or size > 50:
        size = 10
    
    offset = (page - 1) * size
    # åªæŸ¥è¯¢å½“å‰ç”¨æˆ·çš„è®°å½•
    records = db.query(ReasoningRecord).filter(
        ReasoningRecord.user_id == user_id
    ).order_by(ReasoningRecord.last_used_time.desc()).offset(offset).limit(size).all()
    
    history_list = []
    for r in records:
        try:
            keywords_data = json.loads(r.keywords) if r.keywords else []
        except:
            keywords_data = []
        
        try:
            reasoning_steps_data = json.loads(r.reasoning_steps) if r.reasoning_steps else []
        except:
            reasoning_steps_data = []
        
        # è·å–ç»“è®º
        conclusion = ""
        if r.conclusion:
            conclusion = r.conclusion
        else:
            # å¦‚æœæ²¡æœ‰å­˜å‚¨ç»“è®ºï¼Œæ ¹æ®è°£è¨€æ¦‚ç‡ç”Ÿæˆ
            if r.rumor_prob >= 0.7:
                conclusion = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€æ˜¯è°£è¨€ã€‘ã€‚"
            elif r.rumor_prob <= 0.3:
                conclusion = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯ã€ä¸æ˜¯è°£è¨€ã€‘ã€‚"
            else:
                conclusion = "ç»åˆ†æï¼Œè¯¥ä¿¡æ¯å¯èƒ½ä¸ºè°£è¨€ï¼Œå»ºè®®è¿›ä¸€æ­¥æ ¸å®ã€‚"
        
        history_list.append({
            "record_id": r.id,
            "content": r.content,
            "content_hash": r.content_hash,
            "type": r.type,
            "rumor_prob": round(float(r.rumor_prob), 4),
            "is_ai_generated": r.is_ai_generated,
            "keywords": keywords_data,
            "reasoning_steps": reasoning_steps_data,
            "use_count": r.use_count,
            "create_time": r.create_time.strftime("%Y-%m-%d %H:%M:%S") if r.create_time else "",
            "last_used_time": r.last_used_time.strftime("%Y-%m-%d %H:%M:%S") if r.last_used_time else "",
            "conclusion": conclusion
        })
    total = db.query(ReasoningRecord).filter(ReasoningRecord.user_id == user_id).count()
    return {
        "code": 200,
        "msg": "æŸ¥è¯¢æˆåŠŸ",
        "data": {
            "total": total,
            "page": page,
            "size": size,
            "list": history_list
        }
    }
# ---------------------- æŸ¥çœ‹é‡å¤å†…å®¹ç»Ÿè®¡æ¥å£ ----------------------
@app.get("/api/duplicate-stats")
def get_duplicate_stats(
    authorization: str = Header(None),
    db: Session = Depends(get_db)
):
    """æŸ¥çœ‹é‡å¤å†…å®¹ç»Ÿè®¡ä¿¡æ¯"""
    if not authorization or not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="è¯·å…ˆç™»å½•")
    token = authorization.split(" ")[1]
    try:
        user_id = verify_token(token)
    except:
        raise HTTPException(status_code=401, detail="Tokenæ— æ•ˆ/è¿‡æœŸ")
    
    # ç»Ÿè®¡ä½¿ç”¨æ¬¡æ•°æœ€å¤šçš„å†…å®¹ï¼ˆåªç»Ÿè®¡å½“å‰ç”¨æˆ·ï¼‰
    most_used = db.query(ReasoningRecord).filter(
        ReasoningRecord.user_id == user_id
    ).order_by(ReasoningRecord.use_count.desc()).limit(5).all()
    
    most_used_list = []
    for record in most_used:
        most_used_list.append({
            "content": record.content[:50] + "..." if len(record.content) > 50 else record.content,
            "use_count": record.use_count,
            "last_used": record.last_used_time.strftime("%Y-%m-%d %H:%M:%S") if record.last_used_time else "",
            "conclusion": record.conclusion if record.conclusion else ("ã€æ˜¯è°£è¨€ã€‘" if record.rumor_prob >= 0.5 else "ã€ä¸æ˜¯è°£è¨€ã€‘")
        })
    
    # ç»Ÿè®¡ç¼“å­˜å‘½ä¸­ç‡ï¼ˆåªç»Ÿè®¡å½“å‰ç”¨æˆ·ï¼‰
    total_records = db.query(ReasoningRecord).filter(
        ReasoningRecord.user_id == user_id
    ).count()
    
    duplicate_records = db.query(ReasoningRecord).filter(
        ReasoningRecord.user_id == user_id,
        ReasoningRecord.use_count > 1
    ).count()
    
    cache_hit_rate = 0
    if total_records > 0:
        cache_hit_rate = round((duplicate_records / total_records) * 100, 2)
    
    return {
        "code": 200,
        "msg": "ç»Ÿè®¡æˆåŠŸ",
        "data": {
            "total_records": total_records,
            "duplicate_records": duplicate_records,
            "cache_hit_rate": f"{cache_hit_rate}%",
            "most_used_contents": most_used_list
        }
    }

# ---------------------- æ£€æŸ¥ç³»ç»ŸçŠ¶æ€æ¥å£ ----------------------
@app.get("/api/system-status")
def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬æœç´¢åŠŸèƒ½çŠ¶æ€"""
    
    status_info = {
        "duckduckgo_enabled": config.SEARCH_CONFIG.get("enable", True),
        "duckduckgo_available": DUCKDUCKGO_AVAILABLE,
        "user_can_disable_search": config.SEARCH_CONFIG.get("user_can_disable", True),
        "max_results": config.SEARCH_CONFIG.get("max_results", 3),
        "max_queries": config.SEARCH_CONFIG.get("max_queries", 2),
        "timeout": config.SEARCH_CONFIG.get("timeout", 15),
        "llm_model": config.LLM_CONFIG["model_name"],
        "llm_fake_mode": config.LLM_FAKE
    }
    
    # æµ‹è¯•æœç´¢åŠŸèƒ½
    test_result = "æœªæµ‹è¯•"
    if status_info["duckduckgo_enabled"] and status_info["duckduckgo_available"]:
        try:
            test_client = DuckDuckGoSearchClient()
            test_search = test_client.search("æµ‹è¯•", max_results=1)
            test_result = "æ­£å¸¸" if test_search.get("success") else f"å¤±è´¥: {test_search.get('error', 'æœªçŸ¥é”™è¯¯')}"
        except Exception as e:
            test_result = f"å¼‚å¸¸: {str(e)}"
    
    status_info["test_result"] = test_result
    
    return {
        "code": 200,
        "msg": "ç³»ç»ŸçŠ¶æ€æŸ¥è¯¢æˆåŠŸ",
        "data": status_info
    }

# ---------------------- å¯åŠ¨åç«¯ ----------------------
if __name__ == "__main__":
    try:
        jieba.load_userdict('userdict.txt')
        print("âœ… jiebaåˆ†è¯å™¨åˆå§‹åŒ–æˆåŠŸ - åŠ è½½è‡ªå®šä¹‰è¯å…¸")
    except:
        print("âœ… jiebaåˆ†è¯å™¨åˆå§‹åŒ–æˆåŠŸ - ä½¿ç”¨é»˜è®¤è¯å…¸")
    
    # åˆå§‹åŒ–æ•°æ®åº“å’Œæµ‹è¯•ç”¨æˆ·
    db = SessionLocal()
    try:
        if not db.query(User).filter(User.username == "test").first():
            password = str("123456")[:72]
            test_user = User(username="test", password=hash_password(password))
            db.add(test_user)
            db.commit()
            print("âœ… æµ‹è¯•ç”¨æˆ·åˆ›å»ºæˆåŠŸï¼šç”¨æˆ·åtestï¼Œå¯†ç 123456")
        else:
            print("âœ… æµ‹è¯•ç”¨æˆ·å·²å­˜åœ¨")
            
        # æ£€æŸ¥æœç´¢é…ç½®
        print("\n=== DuckDuckGoæœç´¢åŠŸèƒ½çŠ¶æ€ ===")
        print(f"âœ“ å¯ç”¨çŠ¶æ€: {config.SEARCH_CONFIG.get('enable', True)}")
        print(f"âœ“ DuckDuckGoå¯ç”¨: {'æ˜¯' if DUCKDUCKGO_AVAILABLE else 'å¦'}")
        print(f"âœ“ ç”¨æˆ·å¯ç¦ç”¨æœç´¢: {'æ˜¯' if config.SEARCH_CONFIG.get('user_can_disable', True) else 'å¦'}")
        print(f"âœ“ æœ€å¤§ç»“æœæ•°: {config.SEARCH_CONFIG.get('max_results', 3)}")
        print(f"âœ“ æœ€å¤§æŸ¥è¯¢æ•°: {config.SEARCH_CONFIG.get('max_queries', 2)}")
        print(f"âœ“ è¶…æ—¶æ—¶é—´: {config.SEARCH_CONFIG.get('timeout', 15)}ç§’")
        
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")
        db.rollback()
    finally:
        db.close()
    
    print(f"\n=== è°£è¨€ç”„åˆ«ç³»ç»Ÿåç«¯å¯åŠ¨ ===")
    print(f" æ¨¡å‹é…ç½®: {config.LLM_CONFIG['model_name']}")
    print(f" DuckDuckGoæœç´¢: {'å·²å¯ç”¨' if config.SEARCH_CONFIG.get('enable', True) and DUCKDUCKGO_AVAILABLE else 'æœªå¯ç”¨'}")
    print(f" ç”¨æˆ·å¯ç¦ç”¨æœç´¢: {'æ˜¯' if config.SEARCH_CONFIG.get('user_can_disable', True) else 'å¦'}")
    print(f" å»é‡åŠŸèƒ½: å·²å¯ç”¨")
    print(f" LLM_FAKEæ¨¡å¼: {config.LLM_FAKE}")
    print(f" æœåŠ¡åœ°å€: http://localhost:8000")
    print(f" APIæ–‡æ¡£: http://localhost:8000/docs")
    print(f" ç³»ç»ŸçŠ¶æ€æ£€æŸ¥: http://localhost:8000/api/system-status")
    
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)